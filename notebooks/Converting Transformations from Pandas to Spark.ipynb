{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to setup the spark context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the latest spark https://www.apache.org/dyn/closer.lua/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go Inside and run the spark-shell command. This will download all the relevant jars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SparkContext is a client of Spark’s execution environment and it acts as the master of the Spark application. SparkContext sets up internal services and establishes a connection to a Spark execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to avoid hard-coding certain configurations in a SparkConf. For instance, if you’d like to run the same application with different masters or different amounts of memory. Spark allows you to simply create an empty conf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    val sc = new SparkContext(new SparkConf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can supply configuration values at runtime:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./bin/spark-submit --name \"My app\" --master local[4] --conf spark.eventLog.enabled=false --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" myApp.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick word on spark tools: sbt, and spark-submit, spark-shell, pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sbt is the built tool for building scala applications.\n",
    "\n",
    "You will need to submit applications to your spark cluster using the spark-submit.\n",
    "\n",
    "Spark-shell will help you in understanding the code execution flow. It is similar to ipython for spark.\n",
    "\n",
    "FInally the bigger question of whether to use scala. According to me the question is do you already have a lot of legacy code in python, and how comfortable is your team to go into the typed environment of scala. Do you believe that strict typing is your friend, because that will be an extra cognitive load. If you ask me, making the upfront investment in typing will help you in your data debugging process. I highly recommend this is the data that you are getting is ambiguous and is likely to change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between a transformation and action\n",
    "\n",
    "In pandas everything is a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are executed on demand.(Lazy computation)\n",
    "Ex: filter(), union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Action will return a non-RDD type (your stored value types usually)\n",
    "Actions triggers execution using lineage graph to load the data into original RDD\n",
    "Ex: count(), first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things we will be looking at in this hack session.\n",
    "\n",
    "* Creating dataframes.\n",
    "* Get the dataframe shape and columns.\n",
    "* Changing the column names of the dataframes.\n",
    "* Orderby and groupby.\n",
    "* Filtering data.\n",
    "* Membership in dataframe.\n",
    "* Missing value imputation.\n",
    "* Getting a particular data.\n",
    "* Reshaping and Pivoting.\n",
    "* Function application, transformations and mapping.\n",
    "\n",
    "### Things to note.\n",
    "\n",
    "Similarities and differences between the apis of pandas dataframes and spark dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference: creating a pandas DF and a Spark DF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames generally refer to a data structure, which is tabular in nature. It represents rows, each of which consists of a number of observations. Rows can have a variety of data formats (heterogeneous), whereas a column can have data of the same data type (homogeneous). DataFrames usually contain some metadata in addition to data; for example, column and row names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val house_prices_df = spark.read\n",
    "    .format(\"csv\")                                    // this is a csv file.\n",
    "    .option(\"header\", \"true\")                         // the file contains headers\n",
    "    .option(\"inferSchema\", true)                      // read the schema\n",
    "    .load(\"/home/jovyan/data/house-prices/train.csv\") // now load the file.\n",
    "\n",
    "val melb_data = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(\"/home/jovyan/data/melbourne_housing_snapshot/melb_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Taking a look at the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.describe(\"MSSubClass\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities in api\n",
    "\n",
    "| Pandas | Spark   |\n",
    "|------  |------   |\n",
    "| pd.read_csv    | spark.read.load   |\n",
    "| pd.d_types    | df.printSchema   |\n",
    "| df.describe    | df.describe   |\n",
    "| series.unique     | df.select.distinct   |\n",
    "| df.groupby        | df.groupBy   |\n",
    "| filterobject.isin | filterobject.isin   |\n",
    "| df.fillna    | df.na.fill   |\n",
    "| df.sort_values    | df.sort   |\n",
    "| df.merge    | df.join   |\n",
    "| df.append    | df.union   |\n",
    "| df.to_csv    | df.write.format.save(f)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things that are different\n",
    "\n",
    "Some quirks and things to looks out for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframe shape and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of samples is an action and hence be mindful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of features is a no big deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df\n",
    "    .columns\n",
    "    .size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the column names of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq((1L, \"a\", \"foo\", 3.0)).toDF\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newNames = Seq(\"id\", \"x1\", \"x2\", \"x3\")\n",
    "val dfRenamed = df.toDF(newNames: _*)\n",
    "\n",
    "dfRenamed.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val lookup = Map(\"Id\" -> \"id\", \"SalePrice\" -> \"SalePriceDollars\")\n",
    "\n",
    "house_prices_df.columns.map(c => col(c).as(lookup.getOrElse(c, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val lookup = Map(\"Id\" -> \"id\", \"SalePrice\" -> \"SalePriceDollars\")\n",
    "\n",
    "val changed_cols_df = house_prices_df.select(\n",
    "    house_prices_df.columns\n",
    "    .map(\n",
    "        c => col(c).as(lookup.getOrElse(c, c))\n",
    "    ): _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_cols_df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The groupby -> agg -> finalising pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Counts\n",
    "\n",
    "ref: https://stackoverflow.com/a/37949565/5417164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy(\"MSSubClass\")  // groupby your class\n",
    "    .count()                // count the values, this should create a dedicated count column\n",
    "    .orderBy($\"count\" desc)       // orderby the count column\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by and group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy($\"MSSubClass\")                   // Count number of occurrences of each word\n",
    "    .agg(count(\"*\") as \"numOccurances\")       // SQL: SELECT COUNT(DISTINCT MSSubClass) AS numOccurances FROM house_prices_df\n",
    "    .orderBy($\"numOccurances\" desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy($\"MSSubClass\")\n",
    "    .agg(count(\"*\") as \"numOccurances\")\n",
    "    .sort($\"numOccurances\" desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val highSubClass = house_prices_df\n",
    "    .filter($\"MSSubClass\" > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highSubClass.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membership in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val presentList = List(\"20\",\"60\") \n",
    "val nopresentList = List(\"20000\") \n",
    "val validMembership = house_prices_df\n",
    "    .filter($\"MSSubClass\"\n",
    "            .isin(presentList:_*))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val invalidMembership = house_prices_df\n",
    "    .filter($\"MSSubClass\"\n",
    "            .isin(nopresentList:_*))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the count below is the sum of 536 + 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validMembership.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidMembership.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation\n",
    "\n",
    "refs: https://stackoverflow.com/a/40059453/5417164 \n",
    "\n",
    "https://medium.com/@mrpowers/dealing-with-null-in-spark-cfdbb12f231e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to pandas you can replace the na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val imputed_melb_data = melb_data\n",
    "    .na\n",
    "    .fill(1964.0, Seq(\"YearBuilt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_melb_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val features_in_focus = Array(\"Rooms\", \"Bathroom\", \"Landsize\")\n",
    "val features_in_focus_imputed = features_in_focus.map(c => s\"${c}_imputed\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(features_in_focus)\n",
    "  .setOutputCols(features_in_focus_imputed)\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "val imputed_melb_data = imputer.fit(melb_data).transform(melb_data)\n",
    "// val imputed_melb_data = imputer.fit(melb_data2).transform(melb_data2)\n",
    "\n",
    "imputed_melb_data.select(features_in_focus_imputed.map(name => col(name)):_*).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above the features must be for double type or floattype. But the Rooms feature is of type Integer and hence we will need to convert that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "val melb_data2 = melb_data\n",
    "    .withColumn(\n",
    "        \"_Rooms\", melb_data(\"Rooms\").cast(DoubleType))\n",
    "    .drop(\"Rooms\")\n",
    "    .withColumnRenamed(\"_Rooms\", \"Rooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val features_in_focus = Array(\"Rooms\", \"Bathroom\", \"Landsize\")\n",
    "val features_in_focus_imputed = features_in_focus.map(c => s\"${c}_imputed\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(features_in_focus)\n",
    "  .setOutputCols(features_in_focus_imputed)\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "val imputed_melb_data = imputer.fit(melb_data2).transform(melb_data2)\n",
    "\n",
    "imputed_melb_data.select(features_in_focus_imputed.map(name => col(name)):_*).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a particular data\n",
    "\n",
    "ref: https://stackoverflow.com/a/35720457/5417164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = house_prices_df\n",
    "    .filter(line => line(0) == 1)\n",
    "    .select(\"MSSubClass\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result(0)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create RDD for products\n",
    "val data = sc.parallelize(Seq(\n",
    "    (\"memories\",\"book\",\"q1\",10),\n",
    "    (\"dreams\",\"book\",\"q2\",20),\n",
    "    (\"reflections\",\"book\",\"q3\",30),\n",
    "    (\"how to build a house\",\"book\",\"q4\",40),\n",
    "    (\"wonderful life\",\"music\",\"q1\",10),\n",
    "    (\"million miles\",\"music\",\"q2\",20),\n",
    "    (\"run away\",\"music\",\"q3\",30),\n",
    "    (\"mind and body\",\"music\",\"q4\",40)\n",
    "))\n",
    "\n",
    "// convert the RDD to DataFrame\n",
    "val df_products = spark.createDataFrame(data).toDF(\"product\",\n",
    "                                                   \"category\",\n",
    "                                                   \"quarter\",\n",
    "                                                   \"profit\")\n",
    "df_products.show()\n",
    "\n",
    "// index column : category\n",
    "// value column : profit\n",
    "// pivot column : quarter\n",
    "// agg function : sum\n",
    "\n",
    "// apply pivot on DataFrame DataFrame\n",
    "df_products\n",
    "    .groupBy(\"category\")\n",
    "    .pivot(\"quarter\")\n",
    "    .sum(\"profit\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lead, lag}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "import org.apache.spark.sql.types.{StringType, StructType, StructField, IntegerType, FloatType}\n",
    "\n",
    "val schemaStruct = StructType(\n",
    "    StructField(\"date\", IntegerType) ::\n",
    "    StructField(\"low\", FloatType) ::\n",
    "    StructField(\"high\", FloatType) ::\n",
    "    StructField(\"open\", FloatType) ::\n",
    "    StructField(\"close\", FloatType) ::\n",
    "    StructField(\"volume\", FloatType) :: Nil\n",
    ")\n",
    "\n",
    "val crypto_df = spark.read\n",
    "    .format(\"csv\")                                    // this is a csv file.\n",
    "    .schema(schemaStruct)\n",
    "    .load(\"/home/jovyan/data/crypto_data/LTC-USD.csv\")\n",
    "\n",
    "val w = org.apache.spark.sql.expressions.Window.orderBy(\"date\")  \n",
    "\n",
    "val FUTURE_PERIOD_PREDICT = 3\n",
    "\n",
    "val futureDf = crypto_df.withColumn(\"future\", lead(\"close\", FUTURE_PERIOD_PREDICT, 0).over(w))\n",
    "\n",
    "futureDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val targetdf = futureDf.withColumn(\"target\", when($\"future\" > $\"close\", 1).otherwise(0))\n",
    "\n",
    "targetdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetdf.write.format(\"csv\").save(\"/home/jovyan/data/crypto_data/out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function application, transformations and mapping\n",
    "\n",
    "By using user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morePrecision(price: Integer): Float = price.toFloat\n",
    "\n",
    "// we use the method name followed by a \"_\" to indicate we want a reference\n",
    "// to the method, not call it\n",
    "val morePrecisionUdf = udf(morePrecision _)\n",
    "\n",
    "val converted_df = house_prices_df.select($\"SalePrice\", \n",
    "                                          morePrecisionUdf('SalePrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the udf on the same df. ie. creating a new feature by transforming another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df\n",
    "    .withColumn(\"MorePrecisionSalePrice\",\n",
    "                morePrecisionUdf('SalePrice))\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def get_target = udf((future: Float, close: Float) => {\n",
    "  if(future > close) 1\n",
    "  else 0\n",
    "})\n",
    "\n",
    "val target_df = futureDf.withColumn(\"target\", get_target($\"future\", $\"close\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying some transformation on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, upper}\n",
    "\n",
    "val df = sc.parallelize(\n",
    "  Seq((\"a\", \"B\", \"c\"), (\"D\", \"e\", \"F\"))).toDF(\"x\", \"y\", \"z\")\n",
    "df.select(df.columns.map(c => upper(col(c)).alias(c)): _*).show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
